{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised Learning**\n",
    "\n",
    "/**Labels**/: A label is what we are predicting (y) variable in linear regression.\n",
    "\n",
    "/**Features**/: A feature is an input variable (x) in linear regression (x₁,x₂,.....xₙ)\n",
    "\n",
    "/**Examples**/: An example is a particular instance of data, (x->, vector). There are 2 types of examples, labeled and unlabeled examples.\n",
    "\n",
    "/**Labeled Example**/: uses both features and labels: {features, labels}: (x,y)\n",
    "\n",
    "/**Unlabeled Example**/: uses features but not labels: {features, ?}: (x,?)\n",
    "\n",
    "/**Model**/: defines the relationship between features and labels. Model has 2 phases, training and inference.\n",
    "\n",
    "/**Training**/ means creating or learning the model that is to show the model labeled examples and enable to model to gradually learn the relationships between feature and labels\n",
    "\n",
    "/**Inference**/ means applying the trained model to unlabeled examples to make useful predictions.\n",
    "\n",
    "/**Regression Model**/: Predicts continous values\n",
    "\n",
    "/**Classification Model**/: Predicts discrete values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression**\n",
    "\n",
    "Depends upon: y = mx+b where in machine learning is it called y = w₁x₁ + b\n",
    "\n",
    "y is the predicted label, w is the weight, b is the bias and x is the features\n",
    "\n",
    "Example:        y' = b + w₁x₁ + w₂x₂ + w₃x₃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training & Loss**\n",
    "\n",
    "**Training** a model means learning good values for all the weights and the bias from labeled examples. In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss; this process is called **empirical risk minimization**.\n",
    "\n",
    "Loss is the penalty for a bad prediction. **Loss** is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is **zero**; otherwise, the loss is greater. The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples.\n",
    "\n",
    "**Squared Loss** (L₂ Loss)\n",
    "\n",
    "  = the square of the difference between the label and the prediction\n",
    "\n",
    "  = (observation - prediction(x))2\n",
    "  \n",
    "  = (y - y')2\n",
    "\n",
    "**Mean Square Error (MSE)** is the average squared loss per example over the whole dataset. To calculate MSE, sum up all the squared losses for individual examples and then divide by the number of examples:\n",
    "\n",
    "MSE = 1/N Sigma (x,y)E D (y-prediction(x))^2\n",
    "\n",
    "where: \n",
    "\n",
    "* (x,y) is features and labels\n",
    "* prediction(x) is a function of weights and bias in combination with a set of features x.\n",
    "* D is the dataset containing the labeled examples.\n",
    "* N is the number of examples in D\n",
    "\n",
    "In gradient descent, a **batch** is the set of examples you use to calculate the gradient in a single training iteration.\n",
    "\n",
    "**Stochastic gradient descent (SGD)** uses only a single example (a batch size of 1) per iteration to calculate/reduce loss.\n",
    "\n",
    "**Mini-batch stochastic gradient descent (mini-batch SGD)** is a compromise between full-batch iteration and SGD. A mini-batch is typically between 10 and 1,000 examples, chosen at random. Mini-batch SGD reduces the amount of noise in SGD but is still more efficient than full-batch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 1: Simple Linear Regression with Synthetic Data**\n",
    "\n",
    "First we will import relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define functions that build and train a model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(rate):\n",
    "    # A sequential model contains one or more layers.\n",
    "    model = tf.keras.Sequential()\n",
    "    # This line adds one node to one layer in the model\n",
    "    model.add(tf.keras.layers.Dense(units = 1, input_shape = (1,)))\n",
    "    # This line complines the model and trains it to minimise loss\n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=rate),\n",
    "                  loss=\"mean_squared_error\",\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, feature, label, epochs,batch_size = None):\n",
    "    # We give the feature and the label values to the model\n",
    "    # The model will learn for certain number of epochs\n",
    "\n",
    "    history = model.fit(x = feature, y = label, batch_size = batch_size, epochs = epochs)\n",
    "\n",
    "    #This code gathers the trained model's weight and bias\n",
    "    trained_weight = model.get_weights()[0][0]\n",
    "    trained_bias = model.get_weights()[1]\n",
    "\n",
    "    #We can also view the history of epochs run\n",
    "    epochs = history.epoch\n",
    "\n",
    "    #Gather the history (a snapshot) of each epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "\n",
    "    #Gather the model's root mean error\n",
    "    rmse = hist[\"root_mean_squared_error\"]\n",
    "\n",
    "    return trained_weight, trained_bias, epochs, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define plotting functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
    "    # Label the axes.\n",
    "    plt.xlabel(\"feature\")\n",
    "    plt.ylabel(\"label\")\n",
    "\n",
    "    # Plot the feature values vs. label values.\n",
    "    plt.scatter(feature, label)\n",
    "\n",
    "    # Create a red line representing the model. The red line starts\n",
    "    # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
    "    x0 = 0\n",
    "    y0 = trained_bias\n",
    "    x1 = feature[-1]\n",
    "    y1 = trained_bias + (trained_weight * x1)\n",
    "    plt.plot([x0, x1], [y0, y1], c='r')\n",
    "    plt.show()\n",
    "\n",
    "def plot_the_loss(epochs, rmse):\n",
    "    plt.figure()\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Root Mean Squared Error\")\n",
    "\n",
    "    plt.plot(epochs, rmse, label=\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.ylim([rmse.min()*0.97, rmse.max()])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Dataset**\n",
    "\n",
    "Here we have examples of feature and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_feature = np.array([1.0, 2.0,  3.0,  4.0,  5.0,  6.0,  7.0,  8.0,  9.0, 10.0, 11.0, 12.0])\n",
    "my_label   = np.array([5.0, 8.8,  9.6, 14.2, 18.8, 19.5, 21.4, 26.8, 28.9, 32.0, 33.8, 38.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify the Hyperparameters**\n",
    "\n",
    "learning rate, epochs, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "epochs = 500\n",
    "my_batch_size = 12\n",
    "\n",
    "#Train your model now\n",
    "my_model = build_model(learning_rate)\n",
    "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size)\n",
    "\n",
    "\n",
    "plot_the_model(trained_weight,trained_bias, my_feature, my_label )\n",
    "plot_the_loss(epochs, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK TO DO**\n",
    "\n",
    "Try to increase the number of epochs: The graph starts converging\n",
    "\n",
    "Try to increase the learning rate: Red line doesn't align with blue dots and loss curve begins to oscillate\n",
    "\n",
    "Try to adjust the batch_size: Training Items are reduced. Minimum can be set to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of Hyperparameters Tuning**\n",
    "\n",
    "* Training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero.\n",
    "\n",
    "* If the training loss does not converge, train for more epochs.\n",
    "\n",
    "* If the training loss decreases too slowly, increase the learning rate. Note that setting the learning rate too high may also prevent training loss from converging.\n",
    "\n",
    "* If the training loss varies wildly (that is, the training loss jumps around), decrease the learning rate.\n",
    "\n",
    "* Lowering the learning rate while increasing the number of epochs or the batch size is often a good combination.\n",
    "\n",
    "* Setting the batch size to a very small batch number can also cause instability. First, try large batch size values. Then, decrease the batch size until you see degradation.\n",
    "\n",
    "* For real-world datasets consisting of a very large number of examples, the entire dataset might not fit into memory. In such cases, you'll need to reduce the batch size to enable a batch to fit into memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 2: Linear Regression with a Real Dataset**\n",
    "\n",
    "After doing this Colab, you'll know how to do the following:\n",
    "\n",
    "* Read a .csv file into a pandas DataFrame.\n",
    "* Examine a dataset.\n",
    "* Experiment with different features in building a model.\n",
    "* Tune the model's hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the relevant modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# The following lines adjust the granularity of reporting. \n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the .csv file into a pandas DataFrame**\n",
    "\n",
    "You can imagine a pandas DataFrame as a spreadsheet in which each row is identified by a number and each column by a name. Pandas is itself built on another open source Python library called NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset.\n",
    "training_df = pd.read_csv(filepath_or_buffer=\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
    "\n",
    "# Scale the label.\n",
    "training_df[\"median_house_value\"] /= 1000.0\n",
    "\n",
    "# Print the first rows of the pandas DataFrame.D\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examine the dataset**\n",
    "\n",
    "* count: number of rows in that column\n",
    "\n",
    "* mean and std: mean and standard deviation of the values in that column\n",
    "\n",
    "* min and max: lowest and highest value in column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define function and builds and trains the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2(my_learning_rate):\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units = 1, input_shape = (1,)))\n",
    "\n",
    "    model.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate = my_learning_rate),\n",
    "                  loss = \"mean_squared_error\",\n",
    "                  metrics = [tf.keras.metrics.RootMeanSquaredError()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model2(model, df, feature, label, epochs, batch_size):\n",
    "    history = model.fit(x=df[feature],\n",
    "                      y=df[label],\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs)\n",
    "\n",
    "    trained_weight = model.get_weights()[0][0]\n",
    "    trained_bias = model.get_weights()[1]\n",
    "\n",
    "    epochs = history.epoch\n",
    "\n",
    "    hist = pd.DataFrame(history.history)\n",
    "\n",
    "    rmse = hist[\"root_mean_squared_error\"]\n",
    "\n",
    "    return trained_weight, trained_bias, epochs, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Plotting Function**\n",
    "\n",
    "* a scatter plot of the feature vs. the label, and a line showing the output of the trained model\n",
    "* a loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_the_model2(trained_weight, trained_bias, feature, label):\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(label)\n",
    "\n",
    "    #create a scatterplot from 200 random points on the dataset\n",
    "    random_example = training_df.sample(n=200)\n",
    "    plt.scatter(random_example[feature], random_example[label])\n",
    "\n",
    "    x0 = 0\n",
    "    y0 = trained_bias\n",
    "    x1 = random_example[feature].max()\n",
    "    y1 = trained_bias + (trained_weight*x1)\n",
    "    plt.plot([x0,x1],[y0,y1], c='r')\n",
    "    plt.show()\n",
    "\n",
    "def plot_the_loss2(epoch, rmse):\n",
    "    plt.figure()\n",
    "\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"rmse\")\n",
    "\n",
    "    plt.plot(epoch, rmse, label = \"Loss\")\n",
    "    plt.legend()\n",
    "    plt.ylim([rmse.min()*0.97, rmse.max()])\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Call the model function**\n",
    "\n",
    "For now we will use the total_rooms as the most reliant feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 30\n",
    "batch_size = 30\n",
    "\n",
    "my_feature = \"total_rooms\" \n",
    "my_label=\"median_house_value\"\n",
    "\n",
    "my_model = None\n",
    "\n",
    "my_model = build_model(learning_rate)\n",
    "weight, bias, epochs, rmse = train_model(my_model, training_df, \n",
    "                                         my_feature, my_label,\n",
    "                                         epochs, batch_size)\n",
    "\n",
    "print(\"\\nThe learned weight for your model is %.4f\" % weight)\n",
    "print(\"The learned bias for your model is %.4f\\n\" % bias )\n",
    "\n",
    "plot_the_model(weight, bias, my_feature, my_label)\n",
    "plot_the_loss(epochs, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now you can use the model to make predictions**\n",
    "\n",
    "You should make predictions on examples that are not used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(n, feature, label):\n",
    "\n",
    "    batch = training_df[feature][10000:10000 + n]\n",
    "    predicted_values = my_model.predict_on_batch(x=batch)\n",
    "\n",
    "    print(\"feature   label          predicted\")\n",
    "    print(\"  value   value          value\")\n",
    "    print(\"          in thousand$   in thousand$\")\n",
    "    print(\"--------------------------------------\")\n",
    "\n",
    "    for i in range(n):\n",
    "        print (\"%5.0f %6.0f %15.0f\" % (training_df[feature][10000 + i],\n",
    "                                   training_df[label][10000 + i],\n",
    "                                   predicted_values[i][0] ))\n",
    "\n",
    "\n",
    "prediction(25, my_feature, my_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task: Judge the predictive power of the model**\n",
    "\n",
    "Look at the preceding table. How close is the predicted value to the label value? In other words, does your model accurately predict house values?\n",
    "\n",
    "The total_rooms feature had only a little predictive power. Would a different feature have greater predictive power? Try using population as the feature instead of total_rooms.\n",
    "\n",
    "Note: When you change features, you might also need to change the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_feature = \"population\"   # Replace the ? with population or possibly\n",
    "                   # a different column name.\n",
    "\n",
    "# Experiment with the hyperparameters.\n",
    "learning_rate = 0.05\n",
    "epochs = 18\n",
    "batch_size = 3\n",
    "\n",
    "# Don't change anything below this line.\n",
    "my_model = build_model(learning_rate)\n",
    "weight, bias, epochs, rmse = train_model(my_model, training_df, \n",
    "                                         my_feature, my_label,\n",
    "                                         epochs, batch_size)\n",
    "plot_the_model(weight, bias, my_feature, my_label)\n",
    "plot_the_loss(epochs, rmse)\n",
    "\n",
    "prediction(10, my_feature, my_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task to do**\n",
    "\n",
    "You have determined that total_rooms and population were not useful features. That is, neither the total number of rooms in a neighborhood nor the neighborhood's population successfully predicted the median house price of that neighborhood. Perhaps though, the ratio of total_rooms to population might have some predictive power. That is, perhaps block density relates to median house value.\n",
    "\n",
    "To explore this hypothesis, do the following:\n",
    "\n",
    "* Create a synthetic feature that's a ratio of total_rooms to population. (If you are new to pandas DataFrames, please study the Pandas DataFrame Ultraquick Tutorial.)\n",
    "\n",
    "* Tune the three hyperparameters.\n",
    "\n",
    "* Determine whether this synthetic feature produces a lower loss value than any of the single features you tried earlier in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df[\"rooms_per_person\"] = training_df[\"total_rooms\"]/training_df[\"population\"]\n",
    "\n",
    "my_feature = \"rooms_per_person\"\n",
    "\n",
    "learning_rate = 0.05\n",
    "epochs = 25\n",
    "batch_size = 5\n",
    "\n",
    "# Don't change anything below this line.\n",
    "my_model = build_model(learning_rate)\n",
    "weight, bias, epochs, rmse = train_model(my_model, training_df,\n",
    "                                         my_feature, my_label,\n",
    "                                         epochs, batch_size)\n",
    "\n",
    "plot_the_loss(epochs, rmse)\n",
    "prediction(15, my_feature, my_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the loss values, this synthetic feature produces a better model than the individual features you tried in previous tasks. However, the model still isn't creating great predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task to do**\n",
    "\n",
    "Find feature(s) whose raw values correlate with the label\n",
    "\n",
    "So far, we've relied on trial-and-error to identify possible features for the model. Let's rely on statistics instead.\n",
    "\n",
    "A /**correlation matrix**/ indicates how each attribute's raw values relate to the other attributes' raw values. Correlation values have the following meanings:\n",
    "\n",
    "1.0: perfect positive correlation; that is, when one attribute rises, the other attribute rises.\n",
    "-1.0: perfect negative correlation; that is, when one attribute rises, the other attribute falls.\n",
    "0.0: no correlation; the two columns are not linearly related.\n",
    "In general, the higher the absolute value of a correlation value, the greater its predictive power. For example, a correlation value of -0.8 implies far more predictive power than a correlation of -0.2.\n",
    "\n",
    "The following code cell generates the correlation matrix for attributes of the California Housing Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a correlation matrix.\n",
    "training_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generalization: Peril of Overfitting**\n",
    "\n",
    "An overfit model gets a low loss during training but does a poor job predicting new data.\n",
    "\n",
    "William of Ockham, a 14th century friar and philosopher, loved simplicity. He believed that scientists should prefer simpler formulas or theories over more complex ones. To put Ockham's razor in machine learning terms:\n",
    "\n",
    "*The less complex an ML model, the more likely that a good empirical result is not just due to the peculiarities of the sample.*\n",
    "\n",
    "In modern times, we've formalized Ockham's razor into the fields of statistical learning theory and computational learning theory. These fields have developed generalization bounds--a statistical description of a model's ability to generalize to new data based on factors such as:\n",
    "\n",
    "* the complexity of the model\n",
    "* the model's performance on training data\n",
    "\n",
    "One way is to divide your data set into two subsets:\n",
    "\n",
    "* training set—a subset to train a model.\n",
    "* test set—a subset to test the model.\n",
    "\n",
    "\n",
    "**Never train on test data.** If you are seeing surprisingly good results on your evaluation metrics, it might be a sign that you are accidentally training on the test set. For example, high accuracy might indicate that test data has leaked into the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 3: Validation Sets and Test Sets**\n",
    "\n",
    "After doing this Colab, you'll know how to do the following:\n",
    "\n",
    "* Split a training set into a smaller training set and a validation set.\n",
    "\n",
    "* Analyze deltas between training set and validation set results.\n",
    "\n",
    "* Test the trained model with a test set to determine whether your trained model is overfitting.\n",
    "\n",
    "* Detect and fix a common training problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import relevant modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the datasets from the internet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
    "test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scale the label values**\n",
    "\n",
    "The following code cell scales the median_house_value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = 1000.0\n",
    "\n",
    "# Scale the training set's label.\n",
    "train_df[\"median_house_value\"] /= scale_factor \n",
    "\n",
    "# Scale the test set's label\n",
    "test_df[\"median_house_value\"] /= scale_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build and Train the Model Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(my_learning_rate):\n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "  # Most simple tf.keras models are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # Add one linear layer to the model to yield a simple linear regressor.\n",
    "  model.add(tf.keras.layers.Dense(units=1, input_shape=(1,)))\n",
    "\n",
    "  # Compile the model topography into code that TensorFlow can efficiently\n",
    "  # execute. Configure training to minimize the model's mean squared error. \n",
    "  model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=my_learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "  return model               \n",
    "\n",
    "\n",
    "def train_model(model, df, feature, label, my_epochs, \n",
    "                my_batch_size=None, my_validation_split=0.1):\n",
    "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "\n",
    "  history = model.fit(x=df[feature],\n",
    "                      y=df[label],\n",
    "                      batch_size=my_batch_size,\n",
    "                      epochs=my_epochs,\n",
    "                      validation_split=my_validation_split)\n",
    "\n",
    "  # Gather the model's trained weight and bias.\n",
    "  trained_weight = model.get_weights()[0][0]\n",
    "  trained_bias = model.get_weights()[1]\n",
    "\n",
    "  # The list of epochs is stored separately from the \n",
    "  # rest of history.\n",
    "  epochs = history.epoch\n",
    "  \n",
    "  # Isolate the root mean squared error for each epoch.\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  rmse = hist[\"root_mean_squared_error\"]\n",
    "\n",
    "  return epochs, rmse, history.history   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the plotting curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_the_loss_curve(epochs, mae_training, mae_validation):\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Root Mean Squared Error\")\n",
    "\n",
    "    plt.plot(epochs[1:], mae_training[1:], label=\"Training Loss\")\n",
    "    plt.plot(epochs[1:], mae_validation[1:], label=\"Validation Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # We're not going to plot the first epoch, since the loss on the first epoch\n",
    "    # is often substantially greater than the loss for other epochs.\n",
    "    merged_mae_lists = mae_training[1:] + mae_validation[1:]\n",
    "    highest_loss = max(merged_mae_lists)\n",
    "    lowest_loss = min(merged_mae_lists)\n",
    "    delta = highest_loss - lowest_loss\n",
    "    print(delta)\n",
    "\n",
    "    top_of_y_axis = highest_loss + (delta * 0.05)\n",
    "    bottom_of_y_axis = lowest_loss - (delta * 0.05)\n",
    "    \n",
    "    plt.ylim([bottom_of_y_axis, top_of_y_axis])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task: Experiment with the Validation Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.08\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "# Split the original training set into a reduced training set and a\n",
    "# validation set. \n",
    "validation_split = 0.2\n",
    "\n",
    "# Identify the feature and the label.\n",
    "my_feature = \"median_income\"    # the median income on a specific city block.\n",
    "my_label = \"median_house_value\" # the median house value on a specific city block.\n",
    "# That is, you're going to create a model that predicts house value based \n",
    "# solely on the neighborhood's median income.  \n",
    "\n",
    "# Invoke the functions to build and train the model.\n",
    "my_model = build_model(learning_rate)\n",
    "epochs, rmse, history = train_model(my_model, train_df, my_feature, \n",
    "                                    my_label, epochs, batch_size, \n",
    "                                    validation_split)\n",
    "\n",
    "plot_the_loss_curve(epochs, history[\"root_mean_squared_error\"], \n",
    "                    history[\"val_root_mean_squared_error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature engineering** means transforming raw data into a feature vector. Expect to spend significant time doing feature engineering.\n",
    "\n",
    "Many machine learning models must represent the features as real-numbered vectors since the feature values must be multiplied by the model weights.\n",
    "\n",
    "Since models cannot multiply strings by the learned weights, we use feature engineering to convert strings to numeric values.\n",
    "\n",
    "We can accomplish this by defining a mapping from the feature values, which we'll refer to as the vocabulary of possible values, to integers. Since not every street in the world will appear in our dataset, we can group all other streets into a catch-all \"other\" category, known as an **OOV (out-of-vocabulary) bucket.**\n",
    "\n",
    "However, if we incorporate these index numbers directly into our model, it will impose some constraints that might be problematic:\n",
    "\n",
    "* We'll be learning a single weight that applies to all streets. For example, if we learn a weight of 6 for street_name, then we will multiply it by 0 for Charleston Road, by 1 for North Shoreline Boulevard, 2 for Shorebird Way and so on. Consider a model that predicts house prices using street_name as a feature. It is unlikely that there is a linear adjustment of price based on the street name, and furthermore this would assume you have ordered the streets based on their average house price. Our model needs the flexibility of learning different weights for each street that will be added to the price estimated using the other features.\n",
    "\n",
    "* We aren't accounting for cases where street_name may take multiple values. For example, many houses are located at the corner of two streets, and there's no way to encode that information in the street_name value if it contains a single index.\n",
    "\n",
    "To remove both these constraints, we can instead create a binary vector for each categorical feature in our model that represents values as follows:\n",
    "\n",
    "* For values that apply to the example, set corresponding vector elements to 1.\n",
    "\n",
    "* Set all other elements to 0.\n",
    "\n",
    "The length of this vector is equal to the number of elements in the vocabulary. This representation is called a one-hot encoding when a single value is 1, and a multi-hot encoding when multiple values are 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 3: Feature Crosses: Programming Exercise**\n",
    "\n",
    "After doing this Colab, you'll know how to:\n",
    "\n",
    "* Use TensorFlow preprocessing layers to represent features in different ways.\n",
    "* Represent features as bins.\n",
    "* Cross bins to create a feature cross."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load the imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# The following lines adjust the granularity of reporting.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "print(\"Imported the modules.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load, scale, and shuffle the examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
    "test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")\n",
    "\n",
    "# Scale the labels\n",
    "scale_factor = 1000.0\n",
    "# Scale the training set's label.\n",
    "train_df[\"median_house_value\"] /= scale_factor\n",
    "\n",
    "# Scale the test set's label\n",
    "test_df[\"median_house_value\"] /= scale_factor\n",
    "\n",
    "# Shuffle the examples\n",
    "train_df = train_df.reindex(np.random.permutation(train_df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Represent latitude and longitude as floating-point values**\n",
    "\n",
    "This exercise trains on two features using Input layers.\n",
    "\n",
    "A neighborhood's location is typically the most important feature in determining a house's value. The California Housing dataset provides two features, latitude and longitude that identify each neighborhood's location.\n",
    "\n",
    "The following code cell defines two tf.keras.Input layers, one to represent latitude and another one to represent longitude, both as floating-point values.\n",
    "\n",
    "This code cell specifies the features that you'll ultimately train the model on and how each of those features will be represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Input tensors of float values.\n",
    "inputs = {\n",
    "    'latitude':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='latitude'),\n",
    "    'longitude':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='longitude')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define functions that create and train a model, and a plotting function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(my_inputs, my_outputs, my_learning_rate):\n",
    "\n",
    "  model = tf.keras.Model(inputs=my_inputs, outputs=my_outputs)\n",
    "\n",
    "  # Construct the layers into a model that TensorFlow can execute.\n",
    "  model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=my_learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def train_model(model, dataset, epochs, batch_size, label_name):\n",
    "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "\n",
    "  features = {name:np.array(value) for name, value in dataset.items()}\n",
    "  label = np.array(features.pop(label_name))\n",
    "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True)\n",
    "\n",
    "  # The list of epochs is stored separately from the rest of history.\n",
    "  epochs = history.epoch\n",
    "\n",
    "  # Isolate the mean absolute error for each epoch.\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  rmse = hist[\"root_mean_squared_error\"]\n",
    "\n",
    "  return epochs, rmse\n",
    "\n",
    "\n",
    "def plot_the_loss_curve(epochs, rmse):\n",
    "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Root Mean Squared Error\")\n",
    "\n",
    "  plt.plot(epochs, rmse, label=\"Loss\")\n",
    "  plt.legend()\n",
    "  plt.ylim([rmse.min()*0.94, rmse.max()* 1.05])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the model with floating-point representations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.05\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "label_name = 'median_house_value'\n",
    "\n",
    "# The two Input layers are concatenated so they can be passed as a single\n",
    "# tensor to a Dense layer.\n",
    "preprocessing_layer = tf.keras.layers.Concatenate()(list(inputs.values()))\n",
    "dense_output = layers.Dense(units=1, name='dense_layer')(preprocessing_layer)\n",
    "\n",
    "outputs = {\n",
    "  'dense_output': dense_output\n",
    "}\n",
    "\n",
    "# Create and compile the model's topography.\n",
    "my_model = create_model(inputs, outputs, learning_rate)\n",
    "\n",
    "# To view a PNG of this model's layers, uncomment the call to\n",
    "# `tf.keras.utils.plot_model` below. After running this code cell, click\n",
    "# the file folder on the left, then the `my_model.png` file.\n",
    "# tf.keras.utils.plot_model(my_model, \"my_model.png\", show_shapes=True)\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)\n",
    "\n",
    "# Print out the model summary.\n",
    "my_model.summary(expand_nested=True)\n",
    "\n",
    "plot_the_loss_curve(epochs, rmse)\n",
    "\n",
    "print(\"\\n: Evaluate the new model against the test set:\")\n",
    "test_features = {name:np.array(value) for name, value in test_df.items()}\n",
    "test_label = np.array(test_features.pop(label_name))\n",
    "my_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Represent latitude and longitude in buckets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution_in_degrees = 1.0\n",
    "\n",
    "# Create a list of numbers representing the bucket boundaries for latitude.\n",
    "latitude_boundaries = list(np.arange(int(min(train_df['latitude'])),\n",
    "                                     int(max(train_df['latitude'])),\n",
    "                                     resolution_in_degrees))\n",
    "print(\"latitude boundaries: \" + str(latitude_boundaries))\n",
    "\n",
    "# Create a Discretization layer to separate the latitude data into buckets.\n",
    "latitude = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=latitude_boundaries,\n",
    "    name='discretization_latitude')(inputs.get('latitude'))\n",
    "\n",
    "# Number of categories is the length of latitude_boundaries plus one.\n",
    "latitude = tf.keras.layers.CategoryEncoding(\n",
    "    num_tokens=len(latitude_boundaries) + 1,\n",
    "    output_mode='one_hot',\n",
    "    name='category_encoding_latitude')(latitude)\n",
    "\n",
    "# Create a list of numbers representing the bucket boundaries for longitude.\n",
    "longitude_boundaries = list(np.arange(int(min(train_df['longitude'])),\n",
    "                                      int(max(train_df['longitude'])),\n",
    "                                      resolution_in_degrees))\n",
    "\n",
    "print(\"longitude boundaries: \" + str(longitude_boundaries))\n",
    "\n",
    "# Create a Discretization layer to separate the longitude data into buckets.\n",
    "longitude = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=longitude_boundaries,\n",
    "    name='discretization_longitude')(inputs.get('longitude'))\n",
    "\n",
    "# Number of categories is the length of longitude_boundaries plus one.\n",
    "longitude = tf.keras.layers.CategoryEncoding(\n",
    "    num_tokens=len(longitude_boundaries) + 1,\n",
    "    output_mode='one_hot',\n",
    "    name='category_encoding_longitude')(longitude)\n",
    "\n",
    "# Concatenate latitude and longitude into a single tensor as input for the Dense layer.\n",
    "concatenate_layer = tf.keras.layers.Concatenate()([latitude, longitude])\n",
    "\n",
    "dense_output = layers.Dense(units=1, name='dense_layer')(concatenate_layer)\n",
    "\n",
    "# Define an output dictionary we'll send to the model constructor.\n",
    "outputs = {\n",
    "  'dense_output': dense_output\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the model with bucket representations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.04\n",
    "epochs = 35\n",
    "\n",
    "# Build the model.\n",
    "my_model = create_model(inputs, outputs, learning_rate)\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)\n",
    "\n",
    "# Print out the model summary.\n",
    "my_model.summary(expand_nested=True)\n",
    "\n",
    "plot_the_loss_curve(epochs, rmse)\n",
    "\n",
    "print(\"\\n: Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Represent location as a feature cross**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution_in_degrees = 1.0\n",
    "\n",
    "# Create a list of numbers representing the bucket boundaries for latitude.\n",
    "latitude_boundaries = list(np.arange(int(min(train_df['latitude'])),\n",
    "                                     int(max(train_df['latitude'])),\n",
    "                                     resolution_in_degrees))\n",
    "\n",
    "# Create a Discretization layer to separate the latitude data into buckets.\n",
    "latitude = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=latitude_boundaries,\n",
    "    name='discretization_latitude')(inputs.get('latitude'))\n",
    "\n",
    "# Create a list of numbers representing the bucket boundaries for longitude.\n",
    "longitude_boundaries = list(np.arange(int(min(train_df['longitude'])),\n",
    "                                      int(max(train_df['longitude'])),\n",
    "                                      resolution_in_degrees))\n",
    "\n",
    "# Create a Discretization layer to separate the longitude data into buckets.\n",
    "longitude = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=longitude_boundaries,\n",
    "    name='discretization_longitude')(inputs.get('longitude'))\n",
    "\n",
    "# Cross the latitude and longitude features into a single one-hot vector.\n",
    "feature_cross = tf.keras.layers.HashedCrossing(\n",
    "    num_bins=len(latitude_boundaries) * len(longitude_boundaries),\n",
    "    output_mode='one_hot',\n",
    "    name='cross_latitude_longitude')([latitude, longitude])\n",
    "\n",
    "dense_output = layers.Dense(units=1, name='dense_layer')(feature_cross)\n",
    "\n",
    "# Define an output dictionary we'll send to the model constructor.\n",
    "outputs = {\n",
    "  'dense_output': dense_output\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.04\n",
    "epochs = 35\n",
    "\n",
    "# Build the model, this time passing in the feature_cross_feature_layer:\n",
    "my_model = create_model(inputs, outputs, learning_rate)\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)\n",
    "\n",
    "# Print out the model summary.\n",
    "my_model.summary(expand_nested=True)\n",
    "\n",
    "plot_the_loss_curve(epochs, rmse)\n",
    "\n",
    "print(\"\\n: Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
